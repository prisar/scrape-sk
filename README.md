# Scrape-SK

This project contains a collection of Python scripts for scraping and downloading data from various sources, primarily focused on arXiv and Hacker News. The scripts are designed to fetch research papers and articles based on specific topics and keywords.

## Features

- Scrape and download research papers from arXiv based on topic codes.
- Fetch stories from Hacker News that link to `github.io` pages.
- Utilize `gsutil` for interacting with Google Cloud Storage.

## Getting Started

### Prerequisites

- Python 3
- `gsutil` command-line tool (for scripts interacting with Google Cloud Storage)

### Installation

1.  **Clone the repository:**

    ```bash
    git clone https://github.com/your-username/scrape-sk.git
    cd scrape-sk
    ```

2.  **Create and activate a virtual environment:**

    ```bash
    python3 -m venv env
    source env/bin/activate
    ```

3.  **Install the required dependencies:**

    ```bash
    pip install -r requirements.txt
    ```

## Usage

### arXiv Scraper

The `arxiv.py` script scrapes research papers from arXiv based on a predefined list of topic codes.

```bash
python arxiv.py
```

The downloaded PDF files will be saved in the `downloads` directory, organized by topic code.

### Hacker News Scraper

The `hacker_news.py` script fetches stories from Hacker News that contain links to `github.io` pages.

```bash
python hacker_news.py
```

The script will print the title, URL, points, author, and creation date of each story found.

### Google Cloud Storage Scripts

The `arxiv_gsutil.py` and `download_gsutil.py` scripts are used for interacting with Google Cloud Storage.

- `arxiv_gsutil.py`: An example script demonstrating how to use `gsutil` to copy a file from a Google Cloud Storage bucket.
- `download_gsutil.py`: A script that reads a log file from the `data` directory, filters for specific papers, and downloads them from the arXiv dataset on Google Cloud Storage.

To use these scripts, you will need to have `gsutil` configured with your Google Cloud credentials.

## Data

The `data` directory contains log files that are generated by the scraping scripts. These log files can be used for further analysis or to re-download papers.

### arxiv - papers

    python arxiv_links_v2.py

### hackernews - github.io blogs

    python hacker_news.py
